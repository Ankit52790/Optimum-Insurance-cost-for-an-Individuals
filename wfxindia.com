# -*- coding: utf-8 -*-
"""Optimum Insurance cost for an Individuals .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cAm4AGyfbNU866q93KdhfD72FT-QEce0

# **Summary**  

---

# **Optimum Insurance Cost for an Individuals**  

## **Objective**  
This project aims to develop a machine learning model that predicts **individual insurance costs** based on various demographic, health, and lifestyle factors. By analyzing historical data, the model provides a data-driven approach to estimating insurance expenses, helping both insurers and individuals make informed decisions.  

---

## **Dataset Overview**  
- **Total Entries:** 25,000  
- **Number of Features:** 24 (including categorical and numerical variables)  
- **Target Variable:** `insurance_cost` (the amount an individual pays for insurance)  

### **Key Features in the Dataset**  
âœ… **Demographic Variables:** `Age`, `Gender`, `Location`, `Occupation`  
âœ… **Health Metrics:** `BMI`, `Cholesterol Level`, `Average Glucose Level`, `Daily Average Steps`  
âœ… **Lifestyle Factors:** `Smoking Status`, `Alcohol Consumption`, `Exercise Habits`  
âœ… **Medical History:** `Year Last Admitted to Hospital`  
âœ… **Insurance-Specific Features:** `Covered by Any Other Company`  

### **Data Challenges & Missing Values**  
- **`bmi`** had **~990 missing values**, which were filled using the **median**.  
- **`Year_last_admitted`** had **~11,800 missing values**, which were filled with **0** (indicating no hospital admission).  

---

## **Data Preprocessing & Feature Engineering**  
### **Handling Missing Data**  
- `bmi`: Imputed using **median values** to maintain data distribution.  
- `Year_last_admitted`: Replaced NaNs with **0** (assuming no admission if the value is missing).  

### **Encoding Categorical Variables**  
- **Binary Mapping:**  
  - `covered_by_any_other_company`: **Yes (1), No (0)**  
  - `Alcohol`: **Daily (2), Rarely (1), No (0)**  

- **Cholesterol Level Transformation:**  
  - Converted cholesterol ranges (e.g., "180 to 200") into their **midpoints** (e.g., 190).  

- **One-Hot Encoding for Multi-Category Features:**  
  - `Occupation`, `Gender`, `Smoking Status`, `Exercise`, `Location`  

### **Feature Scaling**  
- Standardized continuous features (`age`, `bmi`, `avg_glucose_level`, `daily_avg_steps`) to improve model stability.  

### **Dropped Irrelevant Features**  
- Removed `applicant_id` as it doesnâ€™t contribute to predictions.  

---

## **Exploratory Data Analysis (EDA)**  
### **1. Correlation Analysis**  
- A **correlation heatmap** was generated to identify relationships between features and `insurance_cost`.  
- **Strong Correlations:**  
  - **`age` and `insurance_cost`** (older individuals tend to have higher costs).  
  - **`bmi` and `insurance_cost`** (higher BMI often leads to increased costs).  

### **2. Distribution of Insurance Costs**  
- A histogram showed that `insurance_cost` follows a **right-skewed distribution**, meaning some individuals have exceptionally high costs.  

---

## **Model Development & Performance Evaluation**  
To predict `insurance_cost`, multiple models were trained and tested:  

### **1. Baseline Model: Linear Regression**  
- **Performance Metrics:**  
  - **Mean Absolute Error (MAE):** â‚¹2,722.97  
  - **Mean Squared Error (MSE):** 11,296,207  
  - **R-squared (RÂ²):** 0.9446 (94.46% variance explained)  

- **Observations:**  
  - Linear Regression provided a strong fit but lacked flexibility in capturing **non-linear relationships**.  

### **2. Random Forest Regressor (Ensemble Learning Model)**  
- **Performance Metrics:**  
  - **MAE:** Significantly lower than Linear Regression  
  - **RÂ²:** Improved over Linear Regression  

- **Observations:**  
  - Random Forest performed better due to its ability to handle **non-linearities** and **interactions** among features.  

---

## **Hyperparameter Tuning for Model Optimization**  
To further improve performance, **GridSearchCV** and **RandomizedSearchCV** were used to find the best hyperparameters for Random Forest.  

### **1. GridSearchCV Results (Best Parameters Identified)**  
- `n_estimators`: 100  
- `max_depth`: 20  
- `min_samples_split`: 5  

### **2. RandomizedSearchCV Results (Alternative Best Parameters)**  
- `n_estimators`: 150  
- `max_depth`: 30  
- `min_samples_split`: 2  
- `min_samples_leaf`: 1  

### **Final Model Performance (Optimized Random Forest):**  
| Metric  | GridSearchCV | RandomizedSearchCV |  
|---------|--------------|--------------------|  
| **MAE** | **~2,450**   | ~2,500             |  
| **MSE** | **Lower**    | Slightly Higher    |  
| **RÂ²**  | **~95.2%**   | ~94.8%             |  

- **Conclusion:** GridSearchCV-tuned Random Forest **outperformed** all other models, achieving the best prediction accuracy.  

---

## **Key Insights & Conclusion**  
âœ… **Random Forest outperformed Linear Regression**, showing better prediction accuracy.  
âœ… **Hyperparameter tuning (GridSearchCV) improved performance**, reducing error rates.  
âœ… **Feature engineering played a crucial role** in improving model efficiency.  

### **Future Enhancements**  
ðŸ“Œ **Feature Engineering Improvements:**  
- Incorporate additional **medical history** features.  
- Explore **interaction effects** between lifestyle habits (e.g., smoking + BMI).  

ðŸ“Œ **Advanced Models:**  
- Try **Gradient Boosting Models (XGBoost, LightGBM)** for potentially higher accuracy.  
- Explore **Deep Learning (Neural Networks)** to capture more complex relationships.  

ðŸ“Œ **Real-World Deployment:**  
- Develop a **web app or API** for real-time insurance cost predictions.  
- Implement **explainability tools (SHAP, LIME)** to help users understand cost factors.  

---

## **Final Thoughts**  
This project successfully built a **highly accurate insurance cost prediction model**, leveraging machine learning techniques. The optimized **Random Forest model** provides reliable predictions, making it a valuable tool for **insurance providers** and **individuals** to assess and manage their health-related expenses.  

---

# Step-by-Step Implementation for Predicting Insurance Costs

# Step 1: Importing Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer

"""# Step 2: Load the Data"""

data = pd.read_csv("/content/Data (1).csv")

data.head()

data.info()

"""# Step 3: Data Cleaning and Preprocessing


###  **Data Preprocessing**
- **Handle missing values** (if any).
- **Encode categorical variables** like `Occupation`, `Gender`, `smoking_status`, etc.
- **Feature scaling** for continuous variables like `age`, `bmi`, `avg_glucose_level`, etc.

"""

# 3.1 Handle Missing Values
bmi_imputer = SimpleImputer(strategy='median')
data['bmi'] = bmi_imputer.fit_transform(data[['bmi']])
data['Year_last_admitted'] = data['Year_last_admitted'].fillna(0)

# 3.2 Encode Categorical Variables
# Convert cholesterol level ranges to numeric midpoints
def cholesterol_to_numeric(value):
    parts = value.split(' to ')
    return (int(parts[0]) + int(parts[1])) / 2

data['cholesterol_level'] = data['cholesterol_level'].apply(cholesterol_to_numeric)

# Map binary columns
binary_cols = ['covered_by_any_other_company', 'Alcohol']
binary_mapping = {'Y': 1, 'N': 0, 'Daily': 2, 'Rare': 1, 'No': 0}
for col in binary_cols:
    data[col] = data[col].map(binary_mapping)

# One-hot encode multi-category columns
multi_cat_cols = ['Occupation', 'Gender', 'smoking_status', 'Location', 'exercise']
data = pd.get_dummies(data, columns=multi_cat_cols, drop_first=True)

# Drop irrelevant columns
data = data.drop(columns=['applicant_id'])

"""# Step 4: Split the Data into Training and Testing Sets"""

#  Split the Data into Training and Testing Sets
X = data.drop(columns=['insurance_cost'])
y = data['insurance_cost']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#  Step 5: Exploratory Data Analysis (EDA)

1.   List item
2.   List item


- Understand the **distribution** of variables.
- Identify **correlations** between features and the target variable `insurance_cost`.
- Detect any **outliers** or anomalies.
"""

data.head()

# Step 5: Exploratory Data Analysis (EDA)

# 5.1 Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), cmap='coolwarm', annot=False, fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.show()

# 5.2 Distribution of Insurance Cost
plt.figure(figsize=(8, 5))
sns.histplot(data['insurance_cost'], bins=30, kde=True)
plt.title("Distribution of Insurance Cost")
plt.xlabel("Insurance Cost")
plt.ylabel("Frequency")
plt.show()

"""# Step 6: Model Building and Evaluation
-  Multiple regression models:
  - **Linear Regression**
  - **Ridge/Lasso Regression**
  - **Decision Tree Regressor**
  - **Random Forest Regressor**
  - **Gradient Boosting Regressor**

- Evaluate model performance using metrics like:
  - **Mean Absolute Error (MAE)**
  - **Mean Squared Error (MSE)**
  - **R-squared (RÂ²)**
"""

#  Model Building and Evaluation

# 6.1 Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

y_pred_lr = lr_model.predict(X_test)

# Evaluate Linear Regression
mae_lr = mean_absolute_error(y_test, y_pred_lr)
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print("Linear Regression Performance:")
print(f"MAE: {mae_lr:.2f}")
print(f"MSE: {mse_lr:.2f}")
print(f"R-squared: {r2_lr:.4f}\n")

# 6.2 Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

# Evaluate Random Forest
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print("Random Forest Performance:")
print(f"MAE: {mae_rf:.2f}")
print(f"MSE: {mse_rf:.2f}")
print(f"R-squared: {r2_rf:.4f}\n")

"""### 7. **Model Optimization**
- Tune hyperparameters using **GridSearchCV** or **RandomizedSearchCV**.
"""

# Step 7: Model Optimization using GridSearchCV and RandomizedSearchCV
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=3,
                           scoring='r2',
                           n_jobs=-1)

grid_search.fit(X_train, y_train)

print("Best Parameters from GridSearchCV:")
print(grid_search.best_params_)

param_dist = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

random_search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=42),
                                   param_distributions=param_dist,
                                   n_iter=10,
                                   cv=3,
                                   scoring='r2',
                                   n_jobs=-1,
                                   random_state=42)

random_search.fit(X_train, y_train)

print("Best Parameters from RandomizedSearchCV:")
print(random_search.best_params_)

# Evaluate the optimized Random Forest models
y_pred_best_grid = grid_search.best_estimator_.predict(X_test)
y_pred_best_random = random_search.best_estimator_.predict(X_test)

mae_best_grid = mean_absolute_error(y_test, y_pred_best_grid)
mse_best_grid = mean_squared_error(y_test, y_pred_best_grid)
r2_best_grid = r2_score(y_test, y_pred_best_grid)

mae_best_random = mean_absolute_error(y_test, y_pred_best_random)
mse_best_random = mean_squared_error(y_test, y_pred_best_random)
r2_best_random = r2_score(y_test, y_pred_best_random)

print("Optimized Random Forest Performance (GridSearchCV):")
print(f"MAE: {mae_best_grid:.2f}")
print(f"MSE: {mse_best_grid:.2f}")
print(f"R-squared: {r2_best_grid:.4f}")

print("Optimized Random Forest Performance (RandomizedSearchCV):")
print(f"MAE: {mae_best_random:.2f}")
print(f"MSE: {mse_best_random:.2f}")
print(f"R-squared: {r2_best_random:.4f}")

"""### 8. **Conclusion**
- Select the best-performing model and interpret the results.
"""

# Conclusion
print("\nConclusion:")
print("- Both GridSearchCV and RandomizedSearchCV improved model performance by optimizing hyperparameters.")
print("- The optimized Random Forest model from GridSearchCV achieved slightly better accuracy compared to RandomizedSearchCV.")
print("- RandomizedSearchCV is more efficient for exploring large hyperparameter spaces, whereas GridSearchCV ensures more precise tuning.")
print("- The best-performing model is the GridSearchCV-optimized Random Forest, providing the most reliable insurance cost predictions.")
print("- Further improvements can be made with additional feature engineering and more data.")
